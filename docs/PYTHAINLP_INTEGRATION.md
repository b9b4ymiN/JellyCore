# PyThaiNLP Integration Plan à¸ªà¸³à¸«à¸£à¸±à¸š JellyCore

> **à¸ªà¸–à¸²à¸™à¸°:** à¸§à¸´à¹€à¸„à¸£à¸²à¸°à¸«à¹Œà¹à¸¥à¹‰à¸§ â€” à¸à¸£à¹‰à¸­à¸¡ implement  
> **à¸„à¸³à¸•à¸­à¸šà¸ªà¸±à¹‰à¸™:** à¹ƒà¸Šà¹ˆ à¸„à¸§à¸£à¸™à¸³ PyThaiNLP à¸¡à¸²à¹ƒà¸Šà¹‰à¹ƒà¸™ Oracle V2 à¸­à¸¢à¹ˆà¸²à¸‡à¸¢à¸´à¹ˆà¸‡  
> **à¹€à¸«à¸•à¸¸à¸œà¸¥à¸«à¸¥à¸±à¸:** JellyCore à¹ƒà¸Šà¹‰à¸ à¸²à¸©à¸²à¹„à¸—à¸¢ ~90% à¹à¸•à¹ˆà¹„à¸¡à¹ˆà¸¡à¸µ Thai NLP à¹ƒà¸”à¹† à¹€à¸¥à¸¢ â€” à¸—à¸¸à¸ text processing à¸—à¸³à¹à¸šà¸š naÃ¯ve (split by space/newline) à¸‹à¸¶à¹ˆà¸‡à¹ƒà¸Šà¹‰à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¸à¸±à¸šà¸ à¸²à¸©à¸²à¹„à¸—à¸¢

---

## ğŸ“Š à¸ªà¸£à¸¸à¸›à¸œà¸¥à¸à¸²à¸£à¸¨à¸¶à¸à¸©à¸² PyThaiNLP

| à¸”à¹‰à¸²à¸™ | à¸£à¸²à¸¢à¸¥à¸°à¹€à¸­à¸µà¸¢à¸” |
|------|-----------|
| **Version** | 5.2.0 (stable, actively maintained) |
| **License** | Apache 2.0 â€” à¹ƒà¸Šà¹‰ commercial/personal à¹„à¸”à¹‰à¹€à¸•à¹‡à¸¡à¸—à¸µà¹ˆ |
| **à¸ à¸²à¸©à¸²** | Python 3.9+ |
| **Stars** | 1,100+ / 59 contributors |
| **à¸‚à¸™à¸²à¸”** | `pip install pythainlp` (compact: ~50MB, full: ~500MB) |

### à¸„à¸§à¸²à¸¡à¸ªà¸²à¸¡à¸²à¸£à¸–à¸—à¸µà¹ˆà¹€à¸à¸µà¹ˆà¸¢à¸§à¸‚à¹‰à¸­à¸‡à¸à¸±à¸š JellyCore

| Feature | Module | à¹ƒà¸Šà¹‰à¸—à¸µà¹ˆà¹„à¸«à¸™ | à¸„à¸§à¸²à¸¡à¸ªà¸³à¸„à¸±à¸ |
|---------|--------|----------|----------|
| **Word Tokenization** | `pythainlp.tokenize.word_tokenize` | Oracle search, FTS5 indexing | â˜…â˜…â˜…â˜…â˜… |
| **Sentence Tokenization** | `pythainlp.tokenize.sent_tokenize` | Document chunking | â˜…â˜…â˜…â˜…â˜… |
| **Spell Correction** | `pythainlp.spell.correct` | Search query preprocessing | â˜…â˜…â˜…â˜…â˜† |
| **Stop Words** | `pythainlp.corpus.thai_stopwords` | Search relevance, keyword extraction | â˜…â˜…â˜…â˜…â˜† |
| **Text Normalization** | `pythainlp.util.normalize` | Index preprocessing | â˜…â˜…â˜…â˜…â˜† |
| **Custom Dictionary** | `pythainlp.tokenize.Tokenizer` | Domain-specific terms | â˜…â˜…â˜…â˜†â˜† |
| **Keyboard Correction** | `pythainlp.util.eng_to_thai` | Mistyped queries | â˜…â˜…â˜…â˜†â˜† |
| **Soundex** | `pythainlp.soundex` | Phonetic fuzzy matching | â˜…â˜…â˜†â˜†â˜† |
| **Word Vectors** | `pythainlp.word_vector` | Semantic expansion | â˜…â˜…â˜†â˜†â˜† |
| **Synonyms** | `pythainlp.corpus.thai_synonyms` | Query expansion | â˜…â˜…â˜†â˜†â˜† |

---

## ğŸ” à¸›à¸±à¸à¸«à¸²à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™à¸—à¸µà¹ˆ PyThaiNLP à¹à¸à¹‰à¹„à¸”à¹‰

### à¸›à¸±à¸à¸«à¸² 1: FTS5 à¹„à¸¡à¹ˆà¹€à¸‚à¹‰à¸²à¹ƒà¸ˆà¸ à¸²à¸©à¸²à¹„à¸—à¸¢

Oracle V2 à¹ƒà¸Šà¹‰ SQLite FTS5 à¸ªà¸³à¸«à¸£à¸±à¸š full-text search à¹à¸•à¹ˆ:

```
Query: "à¸­à¸¢à¸²à¸à¸à¸´à¸™à¸‚à¹‰à¸²à¸§à¸œà¸±à¸”à¸à¸¸à¹‰à¸‡"
FTS5 à¸¡à¸­à¸‡à¹€à¸›à¹‡à¸™à¸à¹‰à¸­à¸™à¹€à¸”à¸µà¸¢à¸§: "à¸­à¸¢à¸²à¸à¸à¸´à¸™à¸‚à¹‰à¸²à¸§à¸œà¸±à¸”à¸à¸¸à¹‰à¸‡"
â†’ à¹„à¸¡à¹ˆ match à¸à¸±à¸š document à¸—à¸µà¹ˆà¸¡à¸µà¸„à¸³à¸§à¹ˆà¸² "à¸‚à¹‰à¸²à¸§à¸œà¸±à¸”" à¸«à¸£à¸·à¸­ "à¸à¸¸à¹‰à¸‡" à¹à¸¢à¸à¸à¸±à¸™
```

**PyThaiNLP à¹à¸à¹‰à¹„à¸”à¹‰:**
```python
from pythainlp.tokenize import word_tokenize

word_tokenize("à¸­à¸¢à¸²à¸à¸à¸´à¸™à¸‚à¹‰à¸²à¸§à¸œà¸±à¸”à¸à¸¸à¹‰à¸‡", engine="newmm")
# output: ['à¸­à¸¢à¸²à¸', 'à¸à¸´à¸™', 'à¸‚à¹‰à¸²à¸§à¸œà¸±à¸”', 'à¸à¸¸à¹‰à¸‡']
```

â†’ Index à¹€à¸›à¹‡à¸™ segmented text à¹ƒà¸™ FTS5: `"à¸­à¸¢à¸²à¸ à¸à¸´à¸™ à¸‚à¹‰à¸²à¸§à¸œà¸±à¸” à¸à¸¸à¹‰à¸‡"` â†’ FTS5 MATCH à¸—à¸³à¸‡à¸²à¸™à¹„à¸”à¹‰à¸ˆà¸£à¸´à¸‡

### à¸›à¸±à¸à¸«à¸² 2: Document Chunking à¹„à¸¡à¹ˆà¸£à¸¹à¹‰à¸ˆà¸±à¸à¸›à¸£à¸°à¹‚à¸¢à¸„à¹„à¸—à¸¢

Oracle V2 à¸•à¸±à¸” chunk à¹‚à¸”à¸¢à¹ƒà¸Šà¹‰ `###` headers + bullet points à¹à¸•à¹ˆà¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¸—à¸µà¹ˆà¹„à¸¡à¹ˆà¸¡à¸µ formatting à¸ˆà¸°à¸à¸¥à¸²à¸¢à¹€à¸›à¹‡à¸™à¸à¹‰à¸­à¸™à¸¢à¸²à¸§ à¸šà¸£à¸£à¸—à¸±à¸”à¹€à¸”à¸µà¸¢à¸§à¸­à¸²à¸ˆà¹€à¸›à¹‡à¸™à¸—à¸±à¹‰à¸‡à¸¢à¹ˆà¸­à¸«à¸™à¹‰à¸² (à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¹„à¸¡à¹ˆà¸‚à¸¶à¹‰à¸™à¸šà¸£à¸£à¸—à¸±à¸”à¹ƒà¸«à¸¡à¹ˆà¸—à¸¸à¸à¸›à¸£à¸°à¹‚à¸¢à¸„)

**PyThaiNLP à¹à¸à¹‰à¹„à¸”à¹‰:**
```python
from pythainlp.tokenize import sent_tokenize

text = "à¸‰à¸±à¸™à¹„à¸›à¸›à¸£à¸°à¸Šà¸¸à¸¡à¹€à¸¡à¸·à¹ˆà¸­à¸§à¸²à¸™à¸‚à¹‰à¸²à¸£à¸²à¸Šà¸à¸²à¸£à¹„à¸”à¹‰à¸£à¸±à¸šà¸à¸²à¸£à¸«à¸¡à¸¸à¸™à¹€à¸§à¸µà¸¢à¸™à¹€à¸›à¹‡à¸™à¸£à¸°à¸¢à¸°à¹à¸¥à¸°à¹€à¸‚à¸²à¹„à¸”à¹‰à¸£à¸±à¸šà¸¡à¸­à¸šà¸«à¸¡à¸²à¸¢à¹ƒà¸«à¹‰à¸›à¸£à¸°à¸ˆà¸³à¹ƒà¸™à¸£à¸°à¸”à¸±à¸šà¸ à¸¹à¸¡à¸´à¸ à¸²à¸„"
sent_tokenize(text, engine="crfcut")
# output: ['à¸‰à¸±à¸™à¹„à¸›à¸›à¸£à¸°à¸Šà¸¸à¸¡à¹€à¸¡à¸·à¹ˆà¸­à¸§à¸²à¸™', 'à¸‚à¹‰à¸²à¸£à¸²à¸Šà¸à¸²à¸£à¹„à¸”à¹‰à¸£à¸±à¸šà¸à¸²à¸£à¸«à¸¡à¸¸à¸™à¹€à¸§à¸µà¸¢à¸™à¹€à¸›à¹‡à¸™à¸£à¸°à¸¢à¸°', 'à¹à¸¥à¸°à¹€à¸‚à¸²à¹„à¸”à¹‰à¸£à¸±à¸šà¸¡à¸­à¸šà¸«à¸¡à¸²à¸¢à¹ƒà¸«à¹‰à¸›à¸£à¸°à¸ˆà¸³à¹ƒà¸™à¸£à¸°à¸”à¸±à¸šà¸ à¸¹à¸¡à¸´à¸ à¸²à¸„']
```

â†’ à¹ƒà¸Šà¹‰ `sent_tokenize` à¸£à¹ˆà¸§à¸¡à¸à¸±à¸š overlap chunking (~400 tokens, 80 token overlap) à¹„à¸”à¹‰

### à¸›à¸±à¸à¸«à¸² 3: Search Query à¸œà¸´à¸”/à¸ªà¸°à¸à¸”à¸œà¸´à¸” â†’ à¹„à¸¡à¹ˆà¹€à¸ˆà¸­

à¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸ªà¸°à¸à¸”à¸œà¸´à¸”à¸šà¹ˆà¸­à¸¢ (à¹„à¸¡à¹ˆà¸¡à¸µ spell check built-in) à¹à¸¥à¸° FTS5 à¸•à¹‰à¸­à¸‡à¸à¸²à¸£ exact match:

```
Query: "à¹€à¸«à¸•à¸à¸²à¸£à¸“" (à¸‚à¸²à¸” à¸¸ à¸à¸±à¸š à¹Œ)
FTS5: à¹„à¸¡à¹ˆ match "à¹€à¸«à¸•à¸¸à¸à¸²à¸£à¸“à¹Œ"
```

**PyThaiNLP à¹à¸à¹‰à¹„à¸”à¹‰:**
```python
from pythainlp.spell import correct

correct("à¹€à¸«à¸•à¸à¸²à¸£à¸“")
# output: 'à¹€à¸«à¸•à¸¸à¸à¸²à¸£à¸“à¹Œ'

correct("à¸ªà¸±à¸‡à¹€à¸à¸•à¸¸")
# output: 'à¸ªà¸±à¸‡à¹€à¸à¸•'
```

### à¸›à¸±à¸à¸«à¸² 4: Embedding à¹„à¸¡à¹ˆà¸”à¸µà¹€à¸à¸£à¸²à¸° input à¹„à¸¡à¹ˆ segmented

ChromaDB à¹ƒà¸Šà¹‰ `all-MiniLM-L6-v2` (English-first model) â€” embedding quality à¸ªà¸³à¸«à¸£à¸±à¸šà¸ à¸²à¸©à¸²à¹„à¸—à¸¢à¸•à¹ˆà¸³ à¹€à¸à¸£à¸²à¸°:
1. Model tokenizer à¹„à¸¡à¹ˆà¸£à¸¹à¹‰à¸ˆà¸±à¸à¸ à¸²à¸©à¸²à¹„à¸—à¸¢ â†’ à¹à¸•à¸ character-level
2. à¸‚à¹‰à¸­à¸„à¸§à¸²à¸¡à¹„à¸¡à¹ˆà¹„à¸”à¹‰ segmented â†’ meaning representation à¹„à¸¡à¹ˆà¸”à¸µ

**PyThaiNLP à¸Šà¹ˆà¸§à¸¢à¸šà¸²à¸‡à¸ªà¹ˆà¸§à¸™:**
- Pre-segment text à¸à¹ˆà¸­à¸™à¸ªà¹ˆà¸‡ embedding model â†’ à¸Šà¹ˆà¸§à¸¢ model à¸—à¸µà¹ˆà¸£à¸­à¸‡à¸£à¸±à¸š multilingual
- à¸¥à¸š stop words + normalize â†’ à¸¥à¸” noise à¹ƒà¸™ embedding

> **Note:** à¸›à¸±à¸à¸«à¸²à¸™à¸µà¹‰à¹à¸à¹‰à¹€à¸•à¹‡à¸¡à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™ embedding model à¸”à¹‰à¸§à¸¢ (â†’ Phase 1: multilingual-e5-large)

---

## ğŸ—ï¸ Architecture: PyThaiNLP à¹€à¸›à¹‡à¸™ Sidecar Service

### à¸—à¸³à¹„à¸¡à¹„à¸¡à¹ˆ embed à¸•à¸£à¸‡à¹ƒà¸™ Oracle V2?

| à¸‚à¹‰à¸­à¸à¸´à¸ˆà¸²à¸£à¸“à¸² | à¹€à¸«à¸•à¸¸à¸œà¸¥ |
|------------|--------|
| Oracle V2 à¹€à¸›à¹‡à¸™ Bun/TypeScript | PyThaiNLP à¹€à¸›à¹‡à¸™ Python â€” à¹„à¸¡à¹ˆà¸¡à¸µ native JS port |
| Performance | Python process cold start ~2s, à¹à¸•à¹ˆ warm process = fast |
| Isolation | à¸à¸±à¸‡à¹à¸¢à¸à¸à¸±à¸™ â€” Oracle à¸¢à¸±à¸‡à¸—à¸³à¸‡à¸²à¸™à¹„à¸”à¹‰à¸–à¹‰à¸² PyThaiNLP down |
| Docker | à¹€à¸à¸´à¹ˆà¸¡ container à¸‡à¹ˆà¸²à¸¢ à¹ƒà¸ªà¹ˆ `docker-compose.yml` |

### Design: `thai-nlp-sidecar`

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    JellyCore                         â”‚
â”‚                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    HTTP/JSON    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Oracle V2   â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚ thai-nlp      â”‚  â”‚
â”‚  â”‚  (Bun)       â”‚                â”‚ sidecar       â”‚  â”‚
â”‚  â”‚              â”‚                â”‚ (Python/Flask) â”‚  â”‚
â”‚  â”‚ â€¢ FTS5 index â”‚  /tokenize     â”‚               â”‚  â”‚
â”‚  â”‚ â€¢ ChromaDB   â”‚  /normalize    â”‚ â€¢ PyThaiNLP   â”‚  â”‚
â”‚  â”‚ â€¢ Search     â”‚  /spellcheck   â”‚ â€¢ FastAPI     â”‚  â”‚
â”‚  â”‚ â€¢ Learn      â”‚  /chunk        â”‚ â€¢ ~50MB RAM   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚         â–²                               â–²           â”‚
â”‚         â”‚                               â”‚           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”                       â”‚           â”‚
â”‚  â”‚  NanoClaw     â”‚ (query-router à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡ â”‚           â”‚
â”‚  â”‚  (Node.js)    â”‚  à¹„à¸›à¸–à¸¶à¸‡ sidecar)       â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### API Endpoints à¸‚à¸­à¸‡ Sidecar

```
POST /tokenize
  Body: { "text": "...", "engine": "newmm" }
  Response: { "tokens": ["...", "..."], "segmented": "... ..." }

POST /normalize
  Body: { "text": "..." }
  Response: { "normalized": "...", "changes": [...] }

POST /spellcheck
  Body: { "text": "...", "auto_correct": true }
  Response: { "corrected": "...", "suggestions": [...] }

POST /chunk
  Body: { "text": "...", "max_tokens": 400, "overlap": 80 }
  Response: { "chunks": ["...", "..."], "count": N }

POST /stopwords
  Body: { "tokens": ["...", "..."] }
  Response: { "filtered": ["...", "..."], "removed": ["...", "..."] }

POST /keyboard-fix
  Body: { "text": "Tok8kicsj'xitgmLwmp" }
  Response: { "fixed": "à¸˜à¸™à¸²à¸„à¸²à¸£à¹à¸«à¹ˆà¸‡à¸›à¸£à¸°à¹€à¸—à¸¨à¹„à¸—à¸¢", "was_mistyped": true }

GET /health
  Response: { "status": "ok", "pythainlp_version": "5.2.0" }
```

### Sidecar Implementation (Lightweight)

```python
# thai_nlp_sidecar/main.py
from fastapi import FastAPI
from pythainlp.tokenize import word_tokenize, sent_tokenize, Tokenizer
from pythainlp.spell import correct
from pythainlp.util import normalize
from pythainlp.corpus import thai_stopwords

app = FastAPI(title="Thai NLP Sidecar", version="1.0.0")
STOPWORDS = thai_stopwords()

@app.post("/tokenize")
def tokenize(text: str, engine: str = "newmm"):
    tokens = word_tokenize(text, engine=engine, keep_whitespace=False)
    return {"tokens": tokens, "segmented": " ".join(tokens)}

@app.post("/normalize")
def normalize_text(text: str):
    result = normalize(text)
    return {"normalized": result}

@app.post("/spellcheck")
def spellcheck(text: str, auto_correct: bool = True):
    tokens = word_tokenize(text, keep_whitespace=False)
    corrected = [correct(t) if auto_correct else t for t in tokens]
    return {"corrected": " ".join(corrected), "tokens": corrected}

@app.post("/chunk")
def chunk_text(text: str, max_tokens: int = 400, overlap: int = 80):
    sentences = sent_tokenize(text, engine="crfcut")
    # overlap chunking logic here
    chunks = _overlap_chunk(sentences, max_tokens, overlap)
    return {"chunks": chunks, "count": len(chunks)}

@app.post("/stopwords")
def filter_stopwords(tokens: list[str]):
    filtered = [t for t in tokens if t not in STOPWORDS]
    removed = [t for t in tokens if t in STOPWORDS]
    return {"filtered": filtered, "removed": removed}

@app.get("/health")
def health():
    import pythainlp
    return {"status": "ok", "pythainlp_version": pythainlp.__version__}
```

### Docker Compose Addition

```yaml
# docker-compose.yml (à¹€à¸à¸´à¹ˆà¸¡)
thai-nlp:
  build: ./thai-nlp-sidecar
  restart: unless-stopped
  ports:
    - "47780:8000"
  environment:
    - PYTHAINLP_DATA_DIR=/data/pythainlp
  volumes:
    - ./data/pythainlp:/data/pythainlp
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
    interval: 30s
    timeout: 5s
    retries: 3
  deploy:
    resources:
      limits:
        memory: 256M
```

---

## ğŸ“ Integration Points (à¸ˆà¸¸à¸”à¸—à¸µà¹ˆà¸•à¹‰à¸­à¸‡à¹à¸à¹‰ code)

### à¸ˆà¸¸à¸”à¸—à¸µà¹ˆ 1: Oracle V2 â€” Search Query Preprocessing

**File:** `oracle-v2/src/server/handlers.ts` â†’ `handleSearch()`  
**à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™:**
```typescript
const safeQuery = query.replace(/[?*+\-()^~"':]/g, ' ').replace(/\s+/g, ' ').trim();
```

**à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹€à¸›à¹‡à¸™:**
```typescript
// Step 1: Normalize + spell check via sidecar
const { normalized } = await thaiNlp.normalize(query);
const { corrected } = await thaiNlp.spellcheck(normalized);

// Step 2: Tokenize for FTS5
const { segmented } = await thaiNlp.tokenize(corrected);
const safeQuery = segmented.replace(/[?*+\-()^~"':]/g, ' ').trim();
```

**à¸œà¸¥à¸¥à¸±à¸à¸˜à¹Œ:**
- Query `"à¸­à¸¢à¸²à¸à¸à¸´à¸™à¸‚à¸²à¸§à¸œà¸±à¸”à¸à¸¸à¸‡"` â†’ normalize â†’ spell correct `"à¸­à¸¢à¸²à¸à¸à¸´à¸™à¸‚à¹‰à¸²à¸§à¸œà¸±à¸”à¸à¸¸à¹‰à¸‡"` â†’ tokenize `"à¸­à¸¢à¸²à¸ à¸à¸´à¸™ à¸‚à¹‰à¸²à¸§à¸œà¸±à¸” à¸à¸¸à¹‰à¸‡"` â†’ FTS5 match à¹„à¸”à¹‰

### à¸ˆà¸¸à¸”à¸—à¸µà¹ˆ 2: Oracle V2 â€” Indexing Pipeline

**File:** `oracle-v2/src/server/handlers.ts` â†’ `handleLearn()` + indexing scripts  
**à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™:** Insert raw text à¸¥à¸‡ FTS5

**à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹€à¸›à¹‡à¸™:**
```typescript
// Before inserting to FTS5, segment Thai text
const { segmented } = await thaiNlp.tokenize(content);
const { normalized } = await thaiNlp.normalize(segmented);

// Insert segmented text to FTS5 (so MATCH works with Thai)
sqlite.prepare('INSERT INTO oracle_fts (id, content) VALUES (?, ?)').run(id, normalized);
```

**Critical:** à¸—à¸±à¹‰à¸‡ index time + query time à¸•à¹‰à¸­à¸‡à¹ƒà¸Šà¹‰ tokenizer à¹€à¸”à¸µà¸¢à¸§à¸à¸±à¸™ (`newmm`) à¹„à¸¡à¹ˆà¸‡à¸±à¹‰à¸™ FTS5 MATCH à¸ˆà¸°à¹„à¸¡à¹ˆà¸•à¸£à¸‡à¸à¸±à¸™

### à¸ˆà¸¸à¸”à¸—à¸µà¹ˆ 3: Oracle V2 â€” Document Chunking (à¹ƒà¸«à¸¡à¹ˆ)

**à¹€à¸à¸´à¹ˆà¸¡ chunking pipeline:**
```typescript
// oracle-v2/src/indexer/chunker.ts (à¹„à¸Ÿà¸¥à¹Œà¹ƒà¸«à¸¡à¹ˆ)
async function chunkDocument(content: string): Promise<string[]> {
  const { chunks } = await thaiNlp.chunk(content, {
    max_tokens: 400,
    overlap: 80
  });
  return chunks;
}
```

**à¹ƒà¸Šà¹‰à¸—à¸µà¹ˆ:** indexing pipeline (re-index command + real-time learn)

### à¸ˆà¸¸à¸”à¸—à¸µà¹ˆ 4: NanoClaw â€” Query Router Enhancement (Optional)

**File:** `nanoclaw/src/query-router.ts`  
**à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™:** Regex-based pattern matching â€” à¸—à¸³à¸‡à¸²à¸™à¹„à¸”à¹‰à¸”à¸µà¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§  
**à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™** â€” query router à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡ PyThaiNLP à¹€à¸à¸£à¸²à¸° regex patterns à¸•à¸­à¸™à¸™à¸µà¹‰ match Thai+English à¹„à¸”à¹‰à¸à¸­

> à¸–à¹‰à¸²à¸­à¸™à¸²à¸„à¸•à¸•à¹‰à¸­à¸‡à¸à¸²à¸£ intent classification à¸—à¸µà¹ˆà¸‹à¸±à¸šà¸‹à¹‰à¸­à¸™à¸‚à¸¶à¹‰à¸™ â†’ à¸„à¹ˆà¸­à¸¢à¸à¸´à¸ˆà¸²à¸£à¸“à¸²

### à¸ˆà¸¸à¸”à¸—à¸µà¹ˆ 5: NanoClaw â€” Prompt Builder (Optional)

**File:** `nanoclaw/src/prompt-builder.ts`  
**à¸à¹‡à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™à¹€à¸Šà¹ˆà¸™à¸à¸±à¸™** â€” prompt builder à¸ªà¹ˆà¸‡ query à¹„à¸› Oracle V2 â†’ Oracle à¸—à¸³ preprocessing à¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§

---

## âš¡ Graceful Degradation

PyThaiNLP sidecar down â†’ Oracle V2 à¸•à¹‰à¸­à¸‡à¸—à¸³à¸‡à¸²à¸™à¹„à¸”à¹‰à¸•à¹ˆà¸­:

```typescript
// oracle-v2/src/thai-nlp-client.ts

class ThaiNlpClient {
  private baseUrl: string;
  private timeout: number = 2000; // 2s timeout

  async tokenize(text: string): Promise<{ tokens: string[]; segmented: string }> {
    try {
      const res = await fetch(`${this.baseUrl}/tokenize`, {
        method: 'POST',
        body: JSON.stringify({ text }),
        headers: { 'Content-Type': 'application/json' },
        signal: AbortSignal.timeout(this.timeout)
      });
      return await res.json();
    } catch {
      // Fallback: naive space-based splitting (current behavior)
      const tokens = text.split(/\s+/).filter(Boolean);
      return { tokens, segmented: tokens.join(' ') };
    }
  }

  async normalize(text: string): Promise<{ normalized: string }> {
    try {
      const res = await fetch(`${this.baseUrl}/normalize`, { /* ... */ });
      return await res.json();
    } catch {
      return { normalized: text }; // passthrough
    }
  }

  async spellcheck(text: string): Promise<{ corrected: string }> {
    try {
      const res = await fetch(`${this.baseUrl}/spellcheck`, { /* ... */ });
      return await res.json();
    } catch {
      return { corrected: text }; // passthrough
    }
  }
}
```

â†’ **Zero downtime** â€” à¸–à¹‰à¸² sidecar à¹„à¸¡à¹ˆà¸­à¸¢à¸¹à¹ˆà¸à¹‡à¸à¸¥à¸±à¸šà¹„à¸›à¸—à¸³à¸‡à¸²à¸™à¹à¸šà¸šà¹€à¸”à¸´à¸¡

---

## ğŸ“ Performance Estimates

| Operation | à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™ | + PyThaiNLP | Overhead |
|-----------|---------|-------------|----------|
| Search query preprocessing | ~0ms | ~15-30ms | +15-30ms |
| Document indexing (per doc) | ~5ms | ~20-50ms | +15-45ms |
| Document chunking (per doc) | N/A | ~30-100ms | New |
| Memory (sidecar) | 0 | ~100-200MB | New |

**à¸ªà¸£à¸¸à¸›:** overhead ~15-50ms per search query â€” à¸¢à¸­à¸¡à¸£à¸±à¸šà¹„à¸”à¹‰à¹€à¸à¸£à¸²à¸° Oracle search à¸›à¸à¸•à¸´à¹ƒà¸Šà¹‰ ~100-500ms à¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§

---

## ğŸš€ Implementation Phases

### Phase A: Foundation (1-2 à¸§à¸±à¸™)
1. à¸ªà¸£à¹‰à¸²à¸‡ `thai-nlp-sidecar/` directory
2. à¹€à¸‚à¸µà¸¢à¸™ FastAPI app + Dockerfile
3. à¹€à¸à¸´à¹ˆà¸¡ `docker-compose.yml`
4. à¸ªà¸£à¹‰à¸²à¸‡ `ThaiNlpClient` class à¹ƒà¸™ Oracle V2
5. à¸—à¸”à¸ªà¸­à¸š health check + basic tokenization

### Phase B: Search Enhancement (1-2 à¸§à¸±à¸™)
1. à¹à¸à¹‰ `handleSearch()` â€” à¹€à¸à¸´à¹ˆà¸¡ normalize + spellcheck + tokenize à¸à¹ˆà¸­à¸™ FTS5 MATCH
2. à¹à¸à¹‰ indexing pipeline â€” segment Thai text à¸à¹ˆà¸­à¸™ insert FTS5
3. **à¸ªà¸£à¹‰à¸²à¸‡ re-indexing script** â€” re-index à¹€à¸­à¸à¸ªà¸²à¸£à¹€à¸”à¸´à¸¡à¸—à¸±à¹‰à¸‡à¸«à¸¡à¸”à¸”à¹‰à¸§à¸¢ tokenizer à¹ƒà¸«à¸¡à¹ˆ
4. à¸—à¸”à¸ªà¸­à¸š search quality à¹€à¸—à¸µà¸¢à¸š before/after

### Phase C: Chunking Upgrade (1-2 à¸§à¸±à¸™)
1. à¹€à¸à¸´à¹ˆà¸¡ `/chunk` endpoint à¹ƒà¸™ sidecar
2. à¸ªà¸£à¹‰à¸²à¸‡ `chunker.ts` à¹ƒà¸™ Oracle V2
3. à¹à¸à¹‰ indexing pipeline â€” chunk documents à¸à¹ˆà¸­à¸™ index
4. Re-index à¸”à¹‰à¸§à¸¢ chunking à¹ƒà¸«à¸¡à¹ˆ

### Phase D: Advanced Features (Optional, à¸­à¸™à¸²à¸„à¸•)
1. Custom dictionary à¸ªà¸³à¸«à¸£à¸±à¸š domain-specific terms (à¸Šà¸·à¹ˆà¸­ project, à¸¨à¸±à¸à¸—à¹Œà¹€à¸‰à¸à¸²à¸°)
2. Keyboard correction (`eng_to_thai`) à¸ªà¸³à¸«à¸£à¸±à¸š mistyped queries
3. Query expansion à¸”à¹‰à¸§à¸¢ synonyms
4. Soundex-based fuzzy matching

---

## ğŸ¤” à¸ªà¸´à¹ˆà¸‡à¸—à¸µà¹ˆ PyThaiNLP à¹„à¸¡à¹ˆà¹„à¸”à¹‰à¸Šà¹ˆà¸§à¸¢ (à¸•à¹‰à¸­à¸‡à¸—à¸³à¹€à¸­à¸‡)

| à¸•à¹‰à¸­à¸‡à¸à¸²à¸£ | à¸›à¸±à¸à¸«à¸² | à¸—à¸²à¸‡à¸­à¸­à¸ |
|--------|--------|--------|
| Thai-optimized embeddings | PyThaiNLP à¹„à¸¡à¹ˆà¸¡à¸µ embedding model | à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™ model â†’ multilingual-e5-large (Phase 1) |
| Intent classification | PyThaiNLP classify à¸‡à¹ˆà¸²à¸¢à¹€à¸à¸´à¸™à¹„à¸› | à¹ƒà¸Šà¹‰ LLM (query router à¸›à¸±à¸ˆà¸ˆà¸¸à¸šà¸±à¸™ + à¸­à¸™à¸²à¸„à¸• LLM-based) |
| Semantic search quality | Word tokenization à¸Šà¹ˆà¸§à¸¢à¹„à¸”à¹‰à¸šà¸²à¸‡à¸ªà¹ˆà¸§à¸™ | à¸•à¹‰à¸­à¸‡à¹€à¸›à¸¥à¸µà¹ˆà¸¢à¸™ embedding model à¸”à¹‰à¸§à¸¢ |
| Conversation summarization | à¹„à¸¡à¹ˆà¹ƒà¸Šà¹ˆ scope à¸‚à¸­à¸‡ PyThaiNLP | à¹ƒà¸Šà¹‰ LLM (à¸¡à¸µà¸­à¸¢à¸¹à¹ˆà¹à¸¥à¹‰à¸§) |

---

## âœ… à¸„à¸³à¸•à¸±à¸”à¸ªà¸´à¸™à¹ƒà¸ˆ

| à¸„à¸³à¸–à¸²à¸¡ | à¸„à¸³à¸•à¸­à¸š | à¹€à¸«à¸•à¸¸à¸œà¸¥ |
|-------|-------|--------|
| à¸„à¸§à¸£à¹ƒà¸Šà¹‰ PyThaiNLP à¹„à¸«à¸¡? | **à¹ƒà¸Šà¹ˆ à¸­à¸¢à¹ˆà¸²à¸‡à¸¢à¸´à¹ˆà¸‡** | Thai-first AI à¸•à¹‰à¸­à¸‡à¸¡à¸µ Thai NLP â€” à¹„à¸¡à¹ˆà¸¡à¸µà¸—à¸²à¸‡à¹€à¸¥à¸µà¹ˆà¸¢à¸‡ |
| à¹ƒà¸Šà¹‰à¸—à¸µà¹ˆà¹„à¸«à¸™? | **Oracle V2** (search + indexing) | à¸ˆà¸¸à¸”à¸—à¸µà¹ˆ text processing à¸ªà¸³à¸„à¸±à¸à¸—à¸µà¹ˆà¸ªà¸¸à¸” |
| NanoClaw à¸•à¹‰à¸­à¸‡à¹à¸à¹‰à¹„à¸«à¸¡? | **à¹„à¸¡à¹ˆ** (à¸•à¸­à¸™à¸™à¸µà¹‰) | Query router + prompt builder à¹„à¸¡à¹ˆà¸•à¹‰à¸­à¸‡à¹€à¸‚à¹‰à¸²à¸–à¸¶à¸‡ NLP à¸•à¸£à¸‡ |
| Deploy à¹à¸šà¸šà¹„à¸«à¸™? | **Docker sidecar** | à¹à¸¢à¸ Python process, graceful degradation |
| Engine à¸•à¸±à¸§à¹„à¸«à¸™? | **newmm** (default) | Dictionary-based, thread-safe, best balance accuracy/speed |
| à¸—à¸³à¹€à¸¡à¸·à¹ˆà¸­à¹„à¸«à¸£à¹ˆ? | **Phase 0-1** | Foundation à¸ªà¸³à¸«à¸£à¸±à¸šà¸—à¸¸à¸ search improvement à¸—à¸µà¹ˆà¸ˆà¸°à¸•à¸²à¸¡à¸¡à¸² |

---

## ğŸ“š References

- [PyThaiNLP GitHub](https://github.com/PyThaiNLP/pythainlp) â€” v5.2.0
- [Tokenizer API](https://pythainlp.org/dev-docs/api/tokenize.html) â€” word, sentence, subword
- [Spell API](https://pythainlp.org/dev-docs/api/spell.html) â€” correct, spell, NorvigSpellChecker
- [Corpus API](https://pythainlp.org/dev-docs/api/corpus.html) â€” thai_stopwords, thai_words, thai_synonyms
- [Util API](https://pythainlp.org/dev-docs/api/util.html) â€” normalize, eng_to_thai, countthai
- [BEYOND_OPENCLAW.md](./BEYOND_OPENCLAW.md) â€” JellyCore master improvement plan
