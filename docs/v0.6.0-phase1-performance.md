# v0.6.0 — Phase 1: Performance & Search Intelligence

> **Type:** Major (search quality leap + embedding upgrade + chunking overhaul)  
> **Scope:** Adaptive Hybrid Search + Embedding Model Migration + Overlap Chunking + Indexer Thai NLP  
> **Risk:** Medium-High — changes embedding model (requires full re-index), modifies search ranking algorithm  
> **Est. Effort:** ~2 สัปดาห์ (10-14 วัน)  
> **Prerequisites:** v0.5.0 (Phase 0 — Thai NLP Sidecar + Embedding Versioning) ✅ deployed

---

## Background

Phase 0 วาง foundation — Phase 1 **ใช้ foundation** เพื่อ upgrade search intelligence:

1. **Search ฉลาดขึ้น** — ปัจจุบัน RRF ใช้ weight เท่ากันทุก query → ต้องปรับตาม query type
2. **Embedding เข้าใจภาษาไทย** — `all-MiniLM-L6-v2` ได้ ★★☆☆☆ → ต้องเปลี่ยนเป็น multilingual model
3. **Chunking ดีขึ้น** — ปัจจุบันแบ่งตาม `###` headers → ต้อง overlap + Thai sentence boundary
4. **Indexer ใช้ Thai NLP** — `bun run index` ยังไม่ segment ภาษาไทย (เฉพาะ `handleLearn()` ที่ทำ)
5. **Container Warm Pool** — มีแล้วแต่ยังไม่ optimize (configurable pool sizing + metrics)

> **หลักคิด:** ทำให้ "สมอง" (Oracle V2) ฉลาดขึ้นจริงๆ — ค้นหาตรง, embed เข้าใจ, chunk เหมาะสม

---

## Phase 0 Readiness Verification ✅

ก่อนเริ่ม Phase 1 ได้ตรวจสอบว่า Phase 0 พร้อมครบ:

| # | Success Criteria | สถานะ | หลักฐาน |
|---|-----------------|-------|---------|
| 1 | All 4 Docker services healthy | ✅ Pass | `docker compose ps` → chromadb (healthy), oracle (healthy), nanoclaw (running), thai-nlp (healthy) |
| 2 | Thai NLP tokenization ทำงาน | ✅ Pass | `"อยากกินข้าวผัดกุ้ง"` → `["อยาก","กิน","ข้าวผัด","กุ้ง"]` (0.11ms) |
| 3 | Thai NLP normalization ทำงาน | ✅ Pass | `/normalize` endpoint responds |
| 4 | Thai NLP spellcheck ทำงาน | ✅ Pass | `/spellcheck` corrects Thai text |
| 5 | Thai NLP chunk ทำงาน | ✅ Pass | `/chunk` returns sentence-aware chunks |
| 6 | Thai NLP stopwords ทำงาน | ✅ Pass | Correctly filters `["อยาก","มาก","ที่","จะ"]` as stopwords |
| 7 | Oracle preprocessQuery() ทำงาน | ✅ Pass | `preprocessQuery("อยากกินข้าวผัดกุ้ง")` → segmented output |
| 8 | Graceful degradation (sidecar down) | ✅ Pass | Stopped thai-nlp → Oracle returns original text unchanged, 3 fallback logs |
| 9 | Embedding schema columns exist | ✅ Pass | `embedding_model`, `embedding_version`, `embedding_hash` all present |
| 10 | Embedding versioning data | ✅ Pass | 2 docs have `embedding_model='all-MiniLM-L6-v2'`, `embedding_version=1` |
| 11 | FTS5 re-indexed with Thai segmentation | ✅ Pass | `bun run reindex:thai-nlp` → 2/2 docs re-segmented, 0 errors |
| 12 | `handleLearn()` auto-tokenizes | ✅ Pass | Uses `thaiNlp.normalizeAndTokenize()` before FTS5 insert |

**สรุป: Phase 0 ครบ 12/12 — พร้อมเริ่ม Phase 1**

---

## Changes

### Part A — Adaptive Hybrid Search (จาก Pillar 1.2)

ปัจจุบัน `combineSearchResults()` ใช้ RRF (k=60) กับ recency boost เท่ากันทุก query  
Phase 1 จะ **วิเคราะห์ query ก่อน** แล้วปรับ weight ของ FTS5 vs Vector ตาม query type

#### A.1 — Query Type Classifier

**ไฟล์ใหม่:** `oracle-v2/src/query-classifier.ts`

```typescript
export type QueryType = 'exact' | 'semantic' | 'mixed';

export interface QueryProfile {
  type: QueryType;
  ftsBoost: number;    // 0.0 - 1.0 (RRF weight multiplier for FTS)
  vectorBoost: number; // 0.0 - 1.0 (RRF weight multiplier for Vector)
  reason: string;
}

export function classifySearchQuery(query: string): QueryProfile;
```

**Classification Rules (ไม่ใช้ LLM — rule-based, zero cost):**

| Signal | Query Type | FTS Boost | Vector Boost |
|--------|-----------|-----------|-------------|
| มี backticks / camelCase / ALL_CAPS / error codes | `exact` | **0.8** | 0.2 |
| มี quotes (`"..."`) | `exact` | **0.8** | 0.2 |
| คำถาม (อะไร/ทำไม/อย่างไร/what/why/how) + ยาว >5 words | `semantic` | 0.3 | **0.7** |
| ประโยคยาว >30 chars ไม่มี code markers | `semantic` | 0.3 | **0.7** |
| มีทั้ง code markers + คำถาม | `mixed` | **0.5** | 0.5 |
| Default (ไม่จำแนกได้) | `mixed` | 0.4 | **0.6** |

**Detection Patterns:**
```typescript
const EXACT_SIGNALS = [
  /`[^`]+`/,                           // backticks
  /\b[A-Z][a-z]+[A-Z]\w*/,             // camelCase
  /\b[A-Z]{2,}\b/,                     // ALL_CAPS
  /\b(E[A-Z]+|0x[0-9a-f]+)\b/i,        // error codes
  /["'][^"']{2,}["']/,                  // quoted strings
  /\b\d+\.\d+\.\d+\b/,                 // version numbers (1.2.3)
  /\b(?:port|PORT)\s*[:=]\s*\d+/,       // PORT=47778
];

const SEMANTIC_SIGNALS = [
  /\b(อะไร|ทำไม|อย่างไร|ยังไง|เพราะอะไร)\b/,  // Thai questions
  /\b(what|why|how|when|where|which)\b/i,       // English questions
  /\b(วิธี|แนะนำ|ช่วย|อธิบาย)\b/,              // Thai request words
  /\b(explain|suggest|recommend|help)\b/i,       // English request words
];
```

**LOC est.:** ~80

#### A.2 — Weighted RRF Scoring

**File:** `oracle-v2/src/server/handlers.ts` → `combineSearchResults()`

**ปัจจุบัน:**
```typescript
if (inFts) rrfScore += 1 / (K + ftsRank.get(id)!);
if (inVector) rrfScore += 1 / (K + vectorRank.get(id)!);
```

**เปลี่ยนเป็น:**
```typescript
if (inFts) rrfScore += profile.ftsBoost * (1 / (K + ftsRank.get(id)!));
if (inVector) rrfScore += profile.vectorBoost * (1 / (K + vectorRank.get(id)!));
```

**เหตุผล:** ไม่ต้องเปลี่ยน algorithm ทั้งหมด — แค่ multiply RRF score ด้วย boost factor ตาม query type → minimal change, maximum impact

**LOC est.:** ~15

#### A.3 — Candidate Multiplier Increase

**File:** `oracle-v2/src/server/handlers.ts` → `handleSearch()`

**ปัจจุบัน:** ดึง `limit * 2` จากทั้ง FTS5 และ ChromaDB  
**เปลี่ยนเป็น:** ดึง `limit * 4` (เหมือน OpenClaw `candidateMultiplier: 4`)

```typescript
// Before (Phase 0):
ftsResults = stmt.all(safeQuery, ...projectParams, limit * 2)
// After (Phase 1):
const candidateMultiplier = 4;
ftsResults = stmt.all(safeQuery, ...projectParams, limit * candidateMultiplier)
```

**เหตุผล:** ดึง candidates เยอะขึ้น → RRF merge มี pool ใหญ่ขึ้น → top-K ที่ได้ดีกว่า

**LOC est.:** ~10

#### A.4 — Search Quality Logging

**File:** `oracle-v2/src/server/handlers.ts` → `handleSearch()`

เพิ่ม metadata ใน search response เพื่อ debugging & future dashboard:

```typescript
return {
  results,
  total,
  offset,
  limit,
  mode,
  // Phase 1: Search intelligence metadata
  queryProfile: {
    type: profile.type,        // 'exact' | 'semantic' | 'mixed'
    ftsBoost: profile.ftsBoost,
    vectorBoost: profile.vectorBoost,
    reason: profile.reason,
  },
  timing: {
    totalMs: searchTime,
    ftsMs: ftsTime,
    vectorMs: vectorTime,
    mergeMs: mergeTime,
  },
  candidates: {
    fts: ftsResults.length,
    vector: vectorResults.length,
    merged: combined.length,
  },
  ...(warning && { warning })
};
```

**LOC est.:** ~25

**Part A Subtotal:** ~130 LOC across 4 items

---

### Part B — Embedding Model Upgrade (จาก Pillar 1.4)

เปลี่ยนจาก `all-MiniLM-L6-v2` (384-dim, English-optimized) → `intfloat/multilingual-e5-large` (1024-dim, Thai ★★★★☆)

#### B.1 — Pluggable Embedding Interface

**ไฟล์ใหม่:** `oracle-v2/src/embedder.ts`

```typescript
export interface Embedder {
  readonly modelName: string;
  readonly dimensions: number;
  generate(texts: string[]): Promise<number[][]>;
}

export class MiniLMEmbedder implements Embedder {
  readonly modelName = 'all-MiniLM-L6-v2';
  readonly dimensions = 384;
  // Uses @chroma-core/default-embed (existing behavior)
}

export class MultilingualE5Embedder implements Embedder {
  readonly modelName = 'multilingual-e5-large';
  readonly dimensions = 1024;
  // Uses @xenova/transformers with ONNX model
}

// Factory: select embedder based on env var
export function createEmbedder(): Embedder;
```

**Design decisions:**
- Interface-based → สลับ model ได้ง่าย (config change + re-index)
- `multilingual-e5-large` ใช้ local ONNX → ไม่มี API cost, run ใน Oracle container
- Prefix strategy: e5 ต้อง prefix query ด้วย `"query: "` และ document ด้วย `"passage: "` เพื่อ performance ดีที่สุด
- Sub-batch ที่ 5 (ไม่ใช่ 10 เหมือน MiniLM) เพราะ model ใหญ่กว่า → ใช้ RAM มากกว่า

**LOC est.:** ~120

#### B.2 — Refactor ChromaHttpClient to Use Embedder Interface

**File:** `oracle-v2/src/chroma-http.ts`

**ปัจจุบัน:** Hardcoded `DefaultEmbeddingFunction` จาก `@chroma-core/default-embed`  
**เปลี่ยนเป็น:** รับ `Embedder` interface จาก constructor

```typescript
// Before:
export class ChromaHttpClient {
  private embedder: DefaultEmbeddingFunction;
  constructor(collectionName, baseUrl, authToken) {
    this.embedder = new DefaultEmbeddingFunction();
  }
}

// After:
export class ChromaHttpClient {
  private embedder: Embedder;
  constructor(collectionName, baseUrl, authToken, embedder?: Embedder) {
    this.embedder = embedder || createEmbedder();
  }
}
```

**Critical:** เมื่อเปลี่ยน model → ChromaDB collection ต้อง recreate (dimension เปลี่ยน 384→1024)  
→ ใช้ embedding versioning จาก Phase 0 ตรวจก่อน → ถ้า model เปลี่ยนให้ full re-index อัตโนมัติ

**LOC est.:** ~40

#### B.3 — Re-embed Script (Background Migration)

**ไฟล์ใหม่:** `oracle-v2/scripts/re-embed.ts`

```bash
# Usage:
bun run scripts/re-embed.ts

# What it does:
# 1. Check current model vs EMBEDDING_MODEL env
# 2. If different → delete old ChromaDB collection
# 3. Create new collection with new dimension
# 4. For each doc: embed with new model → store in ChromaDB
# 5. Update embedding_model + embedding_hash in SQLite
# 6. Report stats
```

**ใช้ `EmbeddingCache` จาก Phase 0:**
```typescript
const cache = getEmbeddingCache();
const staleDocs = cache.getStaleDocuments(1000); // Get all stale docs

for (const doc of staleDocs) {
  const content = getFtsContent(doc.id);
  const hash = EmbeddingCache.contentHash(content);

  if (cache.hasCurrentEmbedding(doc.id, hash)) continue; // Skip if already done

  const [embedding] = await embedder.generate([content]);
  await chromaClient.upsertEmbedding(doc.id, embedding, metadata);
  cache.recordEmbedding(doc.id, hash);
}
```

**Safety:**
- Batch size 10 (model ใหญ่กว่า → ใช้ memory เยอะกว่า)
- Progress bar + ETA
- Resume-safe (ใช้ `hasCurrentEmbedding()` check)
- ทำแยกจาก `bun run index` → ไม่กระทบ indexer ปกติ

**LOC est.:** ~100

#### B.4 — Dependencies & Docker

**File:** `oracle-v2/package.json`

```json
{
  "dependencies": {
    "@xenova/transformers": "^2.17.0"  // ONNX Runtime for multilingual-e5-large
  }
}
```

**File:** `oracle-v2/Dockerfile`

- เพิ่ม model download step ใน Docker build (cache ONNX weights)
- เพิ่ม memory limit สำหรับ Oracle container: 256MB → **512MB** (model ต้องการ ~300MB)

**File:** `docker-compose.yml` + `docker-compose.production.yml`

```yaml
oracle:
  deploy:
    resources:
      limits:
        memory: 512M  # Was 256M — multilingual-e5-large needs more RAM
```

**LOC est.:** ~30

**Part B Subtotal:** ~290 LOC across 4 items

---

### Part C — Overlap Chunking with Thai Sentence Boundary (จาก Pillar 1.4)

ปัจจุบัน indexer แบ่ง chunks ตาม `###` headers + bullet points — ได้ chunks ที่:
- บางอันสั้นมาก (1 bullet = 1 chunk, 5-10 words)
- บางอันยาวมาก (1 section = 1 chunk, 500+ words)
- ไม่มี overlap → context หายตรงขอบ chunk

#### C.1 — Smart Chunker Module

**ไฟล์ใหม่:** `oracle-v2/src/chunker.ts`

```typescript
export interface ChunkOptions {
  maxTokens: number;     // Target chunk size (default: 400)
  overlap: number;       // Overlap tokens (default: 80)
  minChunkSize: number;  // Don't split below this (default: 50)
  preserveCodeBlocks: boolean;  // Keep ``` blocks intact (default: true)
  preserveHeaders: boolean;     // Never split mid-header section (default: true)
}

export interface Chunk {
  text: string;
  index: number;         // Chunk number within document
  totalChunks: number;
  tokenCount: number;    // Approximate token count
  startOffset: number;   // Character offset in original text
  endOffset: number;
}

/**
 * Smart document chunker with overlap and Thai sentence awareness.
 * Uses thai-nlp sidecar for sentence tokenization of Thai text.
 */
export class SmartChunker {
  constructor(private options: ChunkOptions = defaultOptions) {}

  /**
   * Split text into overlapping chunks respecting:
   * 1. Code block boundaries (never split a ``` block)
   * 2. Markdown header boundaries (prefer split at ## boundaries)
   * 3. Sentence boundaries (Thai via sidecar, English via regex)
   * 4. Overlap for context continuity at chunk edges
   */
  async chunk(text: string): Promise<Chunk[]>;

  /**
   * Estimate token count for text.
   * For Thai: uses word count from tokenizer (1 Thai word ≈ 1 token)
   * For English: uses whitespace split × 1.3 factor
   */
  estimateTokens(text: string, tokens?: string[]): number;
}
```

**Algorithm:**
```
Input text
    │
    ├── 1. Extract code blocks → preserve as atomic units
    ├── 2. Split by ## headers → section boundaries
    ├── 3. For each section:
    │       ├── If < maxTokens → keep as single chunk
    │       └── If > maxTokens → split by sentences
    │           ├── Thai sentences: via thai-nlp sidecar /chunk endpoint
    │           └── English sentences: regex split at .\s, ?\s, !\s
    ├── 4. Merge small chunks (< minChunkSize) with neighbors
    └── 5. Add overlap windows between adjacent chunks
```

**LOC est.:** ~200

#### C.2 — Integrate Chunker into Indexer

**File:** `oracle-v2/src/indexer.ts`

**ปัจจุบัน `parseResonanceFile()` และ `parseLearningFile()`:**
- Split by `###` / `##` headers
- Each section → 1 document
- Bullets → sub-documents

**เปลี่ยนเป็น:**
```typescript
// มี 2 modes:
// 1. Header-based (existing) → for metadata + ID generation
// 2. SmartChunker (new) → for actual content chunks

// Step 1: Parse headers for structure
const sections = this.parseSections(content, filename);

// Step 2: For each section, smart-chunk for vector search
const chunker = new SmartChunker({ maxTokens: 400, overlap: 80 });
for (const section of sections) {
  const chunks = await chunker.chunk(section.body);
  for (const chunk of chunks) {
    documents.push({
      id: `${section.id}_chunk_${chunk.index}`,
      type: section.type,
      source_file: section.sourceFile,
      content: chunk.text,
      concepts: section.concepts,
      // Chunk metadata for debugging
      chunkIndex: chunk.index,
      totalChunks: chunk.totalChunks,
      parentId: section.id,
    });
  }
}
```

**ผลลัพธ์:**
- ก่อน: 1 section `"### Trust: 500 words..."` → 1 document (500 words, too long for embedding)
- หลัง: 1 section → 2 overlapping chunks (400 words + 80 overlap) → better semantic matching

**LOC est.:** ~60

#### C.3 — Chunk Metadata in Schema

**File:** `oracle-v2/src/db/index.ts`

```sql
ALTER TABLE oracle_documents ADD COLUMN chunk_index INTEGER;
ALTER TABLE oracle_documents ADD COLUMN total_chunks INTEGER;
ALTER TABLE oracle_documents ADD COLUMN parent_id TEXT;
```

**เหตุผล:**
- `chunk_index` + `total_chunks` — ระบุว่า chunk นี้อยู่ตำแหน่งไหนของ document เต็ม
- `parent_id` — link กลับไปหา original section → สามารถ expand context ได้ (ดึง chunk ข้างเคียง)

**LOC est.:** ~15

**Part C Subtotal:** ~275 LOC across 3 items

---

### Part D — Indexer Thai NLP Integration (Gap Fix จาก Phase 0)

Phase 0 ทำให้ `handleLearn()` และ search queries ผ่าน Thai NLP preprocessing แล้ว  
แต่ `bun run index` (main indexer) **ยังไม่ segment ภาษาไทย** → FTS5 ยังเก็บ raw text สำหรับ docs ที่ index ผ่าน indexer

#### D.1 — Add ThaiNlpClient to Indexer

**File:** `oracle-v2/src/indexer.ts` → `storeDocuments()`

**ปัจจุบัน:**
```typescript
insertFts.run(doc.id, doc.content, doc.concepts.join(' '));
```

**เปลี่ยนเป็น:**
```typescript
// Segment Thai text for FTS5 (same pipeline as handleLearn + handleSearch)
const thaiNlp = getThaiNlpClient();
const { segmented } = await thaiNlp.normalizeAndTokenize(doc.content);
insertFts.run(doc.id, segmented, doc.concepts.join(' '));
```

**Critical consistency rule:** ทุกทางที่ text เข้า FTS5 ต้องผ่าน Thai NLP:
1. ✅ `handleSearch()` → `preprocessQuery()` (Phase 0)
2. ✅ `handleConsult()` → `preprocessQuery()` (Phase 0)
3. ✅ `handleLearn()` → `normalizeAndTokenize()` (Phase 0)
4. ⬜ `indexer.storeDocuments()` → `normalizeAndTokenize()` (**Phase 1 นี้**)

**LOC est.:** ~15

#### D.2 — Indexer storeDocuments() Async Refactor

**File:** `oracle-v2/src/indexer.ts` → `storeDocuments()`

ปัจจุบัน `storeDocuments()` ไม่ใช่ async (uses synchronous SQLite) แต่ Thai NLP calls ต้อง await  
→ ต้อง refactor loop เป็น async:

```typescript
// Before:
for (const doc of documents) {
  insertFts.run(doc.id, doc.content, ...);
}

// After:
for (const doc of documents) {
  const { segmented } = await thaiNlp.normalizeAndTokenize(doc.content);
  insertFts.run(doc.id, segmented, ...);
  // Update progress
  this.setIndexingStatus(true, ++processed, documents.length);
}
```

**Performance:** Thai NLP tokenize ใช้ ~0.1-1ms per doc → 5,500 docs ≈ 5-10 วินาทีเพิ่ม  
(ยอมรับได้ — indexing ทำไม่บ่อย)

**LOC est.:** ~25

#### D.3 — Embedding Cache Integration in Indexer

**File:** `oracle-v2/src/indexer.ts` → `storeDocuments()`

Wire embedding cache จาก Phase 0 เข้ากับ indexer เพื่อ:
1. Record embedding metadata ทุกครั้งที่ index
2. Skip re-embedding ถ้า content ไม่เปลี่ยน

```typescript
import { getEmbeddingCache, EmbeddingCache } from './embedding-cache.js';

// In storeDocuments():
const cache = getEmbeddingCache();

for (const doc of documents) {
  const contentHash = EmbeddingCache.contentHash(doc.content);

  // Skip ChromaDB re-embed if content unchanged and model matches
  if (cache.hasCurrentEmbedding(doc.id, contentHash)) {
    skipCount++;
    continue; // Only skip ChromaDB, not SQLite
  }

  // ... embed + store ...
  cache.recordEmbedding(doc.id, contentHash);
}
```

**LOC est.:** ~30

**Part D Subtotal:** ~70 LOC across 3 items

---

### Part E — Container Warm Pool Optimization (จาก Pillar 1, existing)

Container Pool มีแล้วใน `nanoclaw/src/container-pool.ts` แต่ยังขาด:
- Metrics export สำหรับ monitoring
- Adaptive pool sizing ตาม load patterns
- Pre-warm trigger ที่ฉลาดกว่า

#### E.1 — Pool Metrics & Stats Endpoint

**ไฟล์ใหม่:** `nanoclaw/src/pool-metrics.ts`

```typescript
export interface PoolMetrics {
  // Counters
  totalAcquired: number;
  totalColdSpawns: number;
  totalReused: number;
  hitRate: number;          // reused / (reused + coldSpawns)

  // Timing
  avgWarmupMs: number;
  avgAcquireMs: number;

  // Current state
  poolSize: number;
  readyContainers: number;
  inUseContainers: number;

  // Per-group stats
  groupStats: Record<string, {
    acquiredCount: number;
    lastActive: number;
    avgReuse: number;
  }>;
}
```

**File:** `nanoclaw/src/router.ts`

เพิ่ม endpoint `/api/pool/stats` สำหรับ monitoring:

```typescript
app.get('/api/pool/stats', (req, res) => {
  res.json(containerPool.getStats());
});
```

**LOC est.:** ~60

#### E.2 — Predictive Pre-warming

**File:** `nanoclaw/src/container-pool.ts`

ปัจจุบัน pre-warm ทำ manual (`warmForGroup()` ถูกเรียกเมื่อมี message)  
เพิ่ม: **activity-based prediction** — ถ้า group X active ทุกวัน 9AM-11AM → pre-warm ที่ 8:55AM

```typescript
interface GroupActivity {
  folder: string;
  hourlyCount: number[];  // Messages per hour [0-23]
  lastActive: number;
}

/**
 * Analyze activity patterns and pre-warm containers for predicted active groups.
 * Called from warmup interval timer.
 */
private predictAndWarm(): void {
  const currentHour = new Date().getHours();
  const nextHour = (currentHour + 1) % 24;

  for (const [folder, activity] of this.activityLog) {
    if (activity.hourlyCount[nextHour] > 2) { // Historically >2 msgs in next hour
      this.warmForGroup(group, isMain);
    }
  }
}
```

**LOC est.:** ~80

**Part E Subtotal:** ~140 LOC across 2 items

---

## File Changelist Summary

| File | Change | LOC est. | Part |
|------|--------|----------|------|
| **`oracle-v2/src/query-classifier.ts`** (ใหม่) | Query type detection (exact/semantic/mixed) | ~80 | A |
| `oracle-v2/src/server/handlers.ts` | Weighted RRF + candidate multiplier + timing metadata | ~50 | A |
| **`oracle-v2/src/embedder.ts`** (ใหม่) | Pluggable embedding interface + E5 implementation | ~120 | B |
| `oracle-v2/src/chroma-http.ts` | Use Embedder interface instead of hardcoded model | ~40 | B |
| **`oracle-v2/scripts/re-embed.ts`** (ใหม่) | Background embedding migration script | ~100 | B |
| `oracle-v2/package.json` | Add `@xenova/transformers`, bump `0.5.0` → `0.6.0` | ~5 | B |
| `oracle-v2/Dockerfile` | Pre-download E5 ONNX weights, increase memory | ~15 | B |
| `docker-compose.yml` | Oracle memory 512M | ~5 | B |
| `docker-compose.production.yml` | Oracle memory 512M | ~5 | B |
| **`oracle-v2/src/chunker.ts`** (ใหม่) | Smart overlap chunker with Thai sentence boundary | ~200 | C |
| `oracle-v2/src/indexer.ts` | Use SmartChunker + Thai NLP preprocessing | ~100 | C+D |
| `oracle-v2/src/db/index.ts` | Chunk metadata columns migration | ~15 | C |
| **`nanoclaw/src/pool-metrics.ts`** (ใหม่) | Pool metrics tracking | ~60 | E |
| `nanoclaw/src/container-pool.ts` | Predictive pre-warming + activity log | ~80 | E |
| `nanoclaw/src/router.ts` | Pool stats endpoint | ~10 | E |

**Total:** ~905 LOC (new + changed)  
**New files:** 5  
**Services to rebuild:** Oracle V2 (embedding model change) + NanoClaw (pool metrics)  
**Breaking changes:** ChromaDB collection ต้อง recreate (384-dim → 1024-dim)

---

## Dependency Graph

```
                ┌──── A.1 Query Classifier
                │       └──► A.2 Weighted RRF
                │       └──► A.3 Candidate Multiplier
Phase 1         │       └──► A.4 Search Quality Logging
                │
                ├──── B.1 Embedder Interface ──► B.2 ChromaHttpClient Refactor
                │                              ──► B.3 Re-embed Script
                │                              ──► B.4 Docker + Dependencies
                │
                ├──── C.1 Smart Chunker ──► C.2 Indexer Integration
                │                        ──► C.3 Chunk Schema Migration
                │
                ├──── D.1 Indexer Thai NLP ──► D.2 Async Refactor
                │                           ──► D.3 Embedding Cache Wire
                │
                └──── E.1 Pool Metrics
                      E.2 Predictive Pre-warming
```

**Dependencies between parts:**
- A (search) → standalone, ไม่ขึ้นกับ B/C/D
- B (embedding) → ต้องทำ B.1 ก่อน B.2, B.3
- C (chunking) → ต้องทำ C.1 ก่อน C.2; ใช้ Thai NLP `/chunk` (Phase 0)
- D (indexer) → ใช้ Thai NLP client (Phase 0) + Embedding Cache (Phase 0)
- E (pool) → standalone, ไม่ขึ้นกับ A/B/C/D

**Recommended order:**
1. **Day 1-2:** A.1 + A.2 + A.3 + A.4 (Adaptive Hybrid Search — impact สูง, risk ต่ำ)
2. **Day 3-4:** C.1 (Smart Chunker module — complex logic, ต้อง test)
3. **Day 5-6:** B.1 + B.2 (Embedder interface — foundation for model swap)
4. **Day 7-8:** D.1 + D.2 + D.3 (Indexer Thai NLP — gap fix, ต้องทำ)
5. **Day 9-10:** C.2 + C.3 (Integrate chunker into indexer)
6. **Day 11:** B.3 + B.4 (Re-embed script + Docker changes)
7. **Day 12-13:** E.1 + E.2 (Pool optimization — lower priority)
8. **Day 14:** Full re-index + testing + verification

---

## Migration & Deployment Plan

### Step 1: Code Changes (Day 1-13)
Implement all parts in order above. Test locally with `bun test`.

### Step 2: Pre-deployment Backup
```bash
# Backup existing data
docker exec jellycore-oracle cp /data/oracle/oracle.db /data/oracle/oracle.db.pre-v0.6.0
docker exec jellycore-chromadb tar czf /chroma/backup-pre-v0.6.0.tar.gz /chroma/chroma/
```

### Step 3: Deploy
```bash
# Build all changed services
docker compose build oracle nanoclaw

# Deploy (oracle will auto-migrate schema on start)
docker compose up -d
```

### Step 4: Re-index Everything
```bash
# 1. Re-index with new chunking + Thai NLP (FTS5 + SQLite)
docker exec jellycore-oracle bun run index

# 2. Re-embed with new model (ChromaDB)
docker exec jellycore-oracle bun run re-embed
```

**Est. time:** `bun run index` ~30s + `bun run re-embed` ~5-10 min (5,500 docs × E5 embedding)

### Step 5: Verify
```bash
# Test adaptive search with exact query
curl "http://localhost:47778/api/search?q=ECONNREFUSED&mode=hybrid"
# → expect queryProfile.type = 'exact', ftsBoost = 0.8

# Test with semantic query  
curl "http://localhost:47778/api/search?q=วิธี+deploy+Docker&mode=hybrid"
# → expect queryProfile.type = 'semantic', vectorBoost = 0.7

# Check embedding stats
docker exec jellycore-oracle bun -e "
  import { getEmbeddingCache } from './src/embedding-cache.js';
  console.log(getEmbeddingCache().getStats());
"
# → { total: N, current: N, stale: 0, models: { 'multilingual-e5-large': N } }
```

---

## Verification

### Part A — Adaptive Hybrid Search

```bash
# A.1: Query classifier works
curl "http://localhost:47778/api/search?q=handleWebhook&mode=hybrid" | jq '.queryProfile'
# → { "type": "exact", "ftsBoost": 0.8, "vectorBoost": 0.2, "reason": "camelCase" }

# A.1: Thai question detected as semantic
curl "http://localhost:47778/api/search?q=ทำไม+container+ช้า&mode=hybrid" | jq '.queryProfile'  
# → { "type": "semantic", "ftsBoost": 0.3, "vectorBoost": 0.7, "reason": "thai-question" }

# A.3: More candidates returned
curl "http://localhost:47778/api/search?q=trust&mode=hybrid&limit=5" | jq '.candidates'
# → { "fts": 20, "vector": 20, "merged": N }  (was 10)

# A.4: Timing metadata present
curl "http://localhost:47778/api/search?q=test&mode=hybrid" | jq '.timing'
# → { "totalMs": N, "ftsMs": N, "vectorMs": N, "mergeMs": N }
```

### Part B — Embedding Model

```bash
# B.1: New model loaded
docker exec jellycore-oracle bun -e "
  import { createEmbedder } from './src/embedder.js';
  const e = createEmbedder();
  console.log(e.modelName, e.dimensions);
"
# → 'multilingual-e5-large' 1024

# B.3: Re-embed completed
docker exec jellycore-oracle bun run re-embed
# → "Re-embedded N documents with multilingual-e5-large (1024-dim)"

# B: Thai semantic search improved
curl "http://localhost:47778/api/search?q=ความไว้วางใจ&mode=vector&limit=3"
# → should return trust-related principles (was poor with MiniLM)
```

### Part C — Overlap Chunking

```bash
# C.1: Chunks have overlap
docker exec jellycore-oracle bun -e "
  import Database from 'bun:sqlite';
  const db = new Database('/data/oracle/oracle.db', {readonly:true});
  const chunks = db.prepare('SELECT id, chunk_index, total_chunks, parent_id FROM oracle_documents WHERE chunk_index IS NOT NULL LIMIT 5').all();
  console.log(JSON.stringify(chunks, null, 2));
"
# → Shows chunk_index, total_chunks, parent_id for chunked documents

# C: Document count increased (more chunks per section)
curl "http://localhost:47778/api/stats" | jq '.total'
# → should be > previous count (more chunks from overlap)
```

### Part D — Indexer Thai NLP

```bash
# D.1: FTS5 content is segmented after full index
docker exec jellycore-oracle bun -e "
  import Database from 'bun:sqlite';
  const db = new Database('/data/oracle/oracle.db', {readonly:true});
  const sample = db.prepare('SELECT content FROM oracle_fts LIMIT 1').get();
  console.log(sample.content.substring(0, 200));
"
# → Thai text should have spaces between words (segmented by PyThaiNLP)

# D.1: Thai search matches indexed content
curl "http://localhost:47778/api/search?q=ข้าวผัด&mode=fts&limit=3"
# → returns results (was impossible before Phase 0+1)
```

### Part E — Container Warm Pool

```bash
# E.1: Pool stats endpoint
curl "http://localhost:47778/api/pool/stats"  # (or nanoclaw port)
# → { "total": N, "ready": N, "hitRate": 0.X, ... }
```

---

## Graceful Degradation Matrix

| Component Down | ผลกระทบ | Behavior |
|---------------|---------|----------|
| Thai NLP sidecar | FTS5 search = raw text matching | ThaiNlpClient fallback → passthrough |
| ChromaDB | Vector search ไม่ทำงาน | FTS5-only mode (existing) |
| E5 model load fail | Embedding ไม่ทำงาน | Fallback to MiniLM (default-embed) |
| SmartChunker fail | Chunking = เดิม | Fallback to header-based split |
| Query Classifier fail | Search weights = default | Fallback to mixed (0.4/0.6) |

> **หลักการเดิม:** ทุก component ใหม่ต้อง fail gracefully — ไม่เคย crash ระบบทั้งหมด

---

## Performance Impact Analysis

| Component | Before | After | Impact |
|-----------|--------|-------|--------|
| **Search latency** | ~200ms | ~210ms (+classifier) | +5% negligible |
| **Embedding compute** | ~5ms/doc (MiniLM 384) | ~50ms/doc (E5 1024) | +10× per doc (acceptable — batched) |
| **Index time** | ~15s (5,500 docs) | ~25s (+Thai NLP +chunking) | +67% (one-time, acceptable) |
| **Oracle RAM** | ~200MB | ~450MB (E5 model loaded) | +125% → need 512MB limit |
| **ChromaDB storage** | ~8MB (384-dim) | ~22MB (1024-dim) | +175% (still small) |
| **FTS5 accuracy** | ★★☆ (Thai) | ★★★★ (segmented Thai) | Major improvement |
| **Vector accuracy** | ★★☆ (Thai) | ★★★★ (multilingual E5) | Major improvement |
| **Search relevance** | ★★★ (static weights) | ★★★★ (adaptive) | Significant improvement |

---

## Decisions

| Decision | Choice | เหตุผล |
|----------|--------|--------|
| Query classifier = rule-based | ไม่ใช้ LLM classify | Zero cost, <0.1ms, deterministic |
| Weighted RRF (ไม่เปลี่ยน algorithm) | Multiply RRF scores by boost | Minimal code change, preserves existing RRF logic |
| `multilingual-e5-large` (ไม่ใช่ `base`) | Large model | ★★★★ Thai vs ★★★ base — worth extra 200MB RAM |
| E5 ใช้ ONNX local (ไม่ใช่ API) | Free, no network latency | Privacy + cost = 0 + works offline |
| Overlap = 80 tokens | ~20% overlap | Balance between context continuity vs storage cost |
| Chunk size = 400 tokens | ไม่ใช่ 256 หรือ 512 | E5 optimal window, good for both Thai+English |
| Indexer Thai NLP = sync per-doc | ไม่ batch | Simpler code, 0.1ms per doc is fast enough |
| Pool prediction = hourly histogram | ไม่ใช้ ML | Simple, good enough for personal AI with 1-2 users |
| Candidate multiplier = 4× | เหมือน OpenClaw | More candidates → better RRF merge |
| Part E = lower priority | ทำท้ายสุด | Pool already works — optimization ไม่ urgent |

---

## Rollback Plan

| Situation | Action |
|-----------|--------|
| E5 model ใช้ RAM เกินไป | Set `EMBEDDING_MODEL=all-MiniLM-L6-v2` → restart → auto-fallback to MiniLM |
| Chunking ทำ search แย่ลง | Revert `indexer.ts` chunking changes → `bun run index` → back to header-based |
| Adaptive search ผิดพลาด | Disable classifier: set all queries to `mixed` default weights |
| ChromaDB dimension mismatch | Delete collection → `bun run re-embed` → recreate with correct dimension |
| Full rollback | Restore `oracle.db.pre-v0.6.0` + old Docker image → `docker compose up -d` |

---

## Success Criteria

- [ ] **Adaptive Search:** Query `"handleWebhook"` classified as `exact` (ftsBoost=0.8)
- [ ] **Adaptive Search:** Query `"ทำไม container ช้า"` classified as `semantic` (vectorBoost=0.7)
- [ ] **Adaptive Search:** Search response includes `queryProfile` + `timing` metadata
- [ ] **Embedding:** `getEmbeddingCache().getStats()` shows `model: 'multilingual-e5-large'`, stale=0
- [ ] **Embedding:** Thai semantic query `"ความไว้วางใจ"` returns trust-related principles (>0.6 similarity)
- [ ] **Chunking:** Documents with >400 tokens split into overlapping chunks
- [ ] **Chunking:** Code blocks preserved intact (not split mid-block)
- [ ] **Chunking:** `chunk_index`, `total_chunks`, `parent_id` populated in schema
- [ ] **Indexer:** `bun run index` produces segmented Thai text in FTS5
- [ ] **Indexer:** FTS5 search `"ข้าวผัด"` matches documents containing `"ข้าวผัดกุ้ง"`
- [ ] **Pool:** `/api/pool/stats` returns metrics with `hitRate` field
- [ ] **Graceful:** E5 model load failure → falls back to MiniLM without crash
- [ ] **Graceful:** Thai NLP sidecar down → indexer completes with raw text
- [ ] **Performance:** Search latency < 500ms p95 (was ~200ms — allow 300ms headroom)
- [ ] **Performance:** Full re-index completes in < 2 minutes

---

## What's Next: Phase 2 Preview

Phase 1 upgrades "สมอง" → Phase 2 **harden สถาปัตยกรรม**:

```
Phase 2 (Week 3-4): Architecture Hardening
├── 2.8 Multi-Provider Failover Chain (Pillar 3.1)
│   └── Sonnet → Haiku → local model chain
│   └── Provider cooldown + auto-rotation
├── 2.9 Auth Profile Rotation (Pillar 3.2)  
│   └── Multiple API keys per provider
│   └── Rate limit detection + key switch
└── Container Architecture Hardening
    └── Resource limits enforcement
    └── Container health monitoring
```

---

*ไฟล์นี้เป็นส่วนหนึ่งของ [BEYOND_OPENCLAW.md](./BEYOND_OPENCLAW.md) implementation roadmap*  
*ต่อจาก [v0.5.0-phase0-foundation.md](./v0.5.0-phase0-foundation.md) ที่ deploy เรียบร้อยแล้ว*  
*ดู PyThaiNLP details ที่ [PYTHAINLP_INTEGRATION.md](./PYTHAINLP_INTEGRATION.md)*
